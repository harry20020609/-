# 数据科学大作业

## 1.情况

现在程序部分是除了爬虫以外的部分已经全部完成了。大概花了我一整周的时间。（爬虫部分我打算期末考试结束之后，有兴趣再做）

如果你们中有人能够做出改进的话，那就太好了！（我写出来的东西我都不想再看第二遍。。。）

我自己使用下来的情况不是特别好，主要是jieba分词中没有完整的语料库和词典——如果想要自己编写，花的时间有点长。

后面如果有空的话我会再着手改进。

还是能够大致做一个分词。

==接下来你们的任务就是**合作**把研究报告做完，以及演示视频的录制==

==研究报告就是从文书网上挑文书，然后进行批注分析，最后进行一个人工的数据统计。==

==DDL就定在这个作业本身截止的时间==

关于报告的具体要求 我正在问助教，等有消息了我会说。

## 2.使用软件前的准备（如果打算人工批注 后面都没必要看了）

安装==jieba，eel==两个库

安装方法很简单，在命令行中输入
$$
pip\  install \ jieba\\
pip\  install \ eel
$$
等待安装完毕后就可以了。

## 3.使用软件

解压好了之后 拿pycharm（随便哪个能运行.py文件的ide都可以）打开整个文件夹

然后直接运行==main.py==文件

应该会弹出一个浏览器页面。

![image-20211207113936659](C:\Users\SeedOil\AppData\Roaming\Typora\typora-user-images\image-20211207113936659.png)

大概像这样。

### 1）文本输入

因为爬虫还没有实现 所以只能手动输入。

手动输入也有两种方法：

1.如果本地有文书的txt文件，就可以点击**”选择文件“**按钮，然后选定文件。

再点击一下**“读取文本文件”**，接下来文件中的文本就会直接显现在那个大文本框中。

2.没有本地文件，那么就直接把文书上的文本**复制粘贴**到那个大文本框中。

### 2）批注分析

直接点击右上角那个绿色的**“开始批注”**按钮就可以了。

### 3）选择有用的分词数据

在下拉框中有各种不同的词性，去找能够符合==**要求的犯人信息**==的词语。

然后点击**分析选择的数据**

### 4）人为处理

在右边那个**犯人信息提取**框中可以直接更改其中的信息。

尽量把它改得符合意义就可以了。

### 5）保存

点击**“保存”**按钮就可以了。

然后会下载两个文件：

1.".json"包括了犯人信息提取的信息

2.".txt"包括了文书所有的内容

至此就全部结束了。